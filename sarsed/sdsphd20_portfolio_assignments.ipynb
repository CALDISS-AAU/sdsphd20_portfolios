{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sdsphd20_portfolio-assignments.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "883tPg8mDseq"
      },
      "source": [
        "# Portfolio for SDSPhD20\n",
        "\n",
        "This notebook contains the exercises and assignments to be answered in a portfolio for the PhD course \"Social Data Science: An Applied Introduction to Machine Learning\" at Aalborg University, November 2020.\n",
        "\n",
        "Each day of the course you are given an hour to work on a portfolio with the possibility of sparring with the course lecturers. \n",
        "\n",
        "You are expected to attempt to solve the various assignments using the methods and tools taught during the course. Answers should be combined into a notebook (fx by adding answers to a copy of this one). \n",
        "\n",
        "**Note:** You are not expected to attempt to solve every single assignment. Note the different requirements for each day.\n",
        "\n",
        "#### How to hand in your portfolio notebooks\n",
        "\n",
        "You can hand in your portfolio notebooks in two ways:\n",
        "\n",
        "- Saving your notebooks in a GitHub repository and then sending the repository URL to the course organizer (Kristian Kjelmann)\n",
        "- Sharing your notebooks directly with the course organizer (Kristian Kjelmann) in Google Colab.\n",
        "\n",
        "Kristianâ€™s e-mail: kgk@adm.aau.dk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDgEJEMSDser"
      },
      "source": [
        "# Portfolio assignments for Tuesday (unsupervised and supervised machine learning)\n",
        "\n",
        "**Requirement**: Work on solutions for *either* \"unsupervised machine learning with penguins\" or \"clustering\" *and* *either* \"supervised machine learning with penguins\" or \"employee turnover\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sms28oD6Dser"
      },
      "source": [
        "---\n",
        "\n",
        "## Unsupervised machine learning with penguins\n",
        "\n",
        "The palmer penguin dataset is excellent for EDA and UML. It contains different measures for 3 species of closely related penguins from several islands in Antarctica.\n",
        "\n",
        "Let's have a look:\n",
        "\n",
        "Penguin datast: https://github.com/allisonhorst/palmerpenguins\n",
        "![penguins](https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/lter_penguins.png)\n",
        "\n",
        "![penguin_beaks](https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/culmen_depth.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRtYQkhoDser"
      },
      "source": [
        "### The assignment\n",
        "\n",
        "1. Inspect the data with some of the standard functions you learned so far (desc, info etc.). What do we have here?\n",
        "2. Likewise, use some standard visualizations (eg. from seaborn) to express some properties of the data\n",
        "3. Create a new dataset where you scale all numeric values with the standardscaler.\n",
        "4. Perform a PCA analysis\n",
        "5. Investigate the explained variance of the components... do we see an 'elbow'?\n",
        "5. Plot the data in the space of the first two components. Maybe color it by species or island. What pattern do we see?\n",
        "6. Inspect the correlation between the components. Which variables are they mostly associated with?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-CSaYomDser"
      },
      "source": [
        "#### Data and libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gWx05FPDser"
      },
      "source": [
        "# standard packaging\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.set(color_codes=True, rc={'figure.figsize':(10,8)})\n",
        "\n",
        "from IPython.display import HTML #Youtube embed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yusQXhgyDseu"
      },
      "source": [
        "# load the dataset from GitHub - original source\n",
        "\n",
        "penguins = pd.read_csv(\"https://github.com/allisonhorst/palmerpenguins/raw/5b5891f01b52ae26ad8cb9755ec93672f49328a8/data/penguins_size.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyAAe803Dseu"
      },
      "source": [
        "# You solutions from here..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obd04ZJGDseu"
      },
      "source": [
        "---\n",
        "\n",
        "## Clustering\n",
        "\n",
        "I have created a larger set of variables from the Danish Value Study from 1999. You can find data here:\n",
        "\n",
        "https://raw.githubusercontent.com/CALDISS-AAU/sdsphd20/master/datasets/value99.csv\n",
        "\n",
        "In all examples, values towards 1 is agree a lot and values towards 5 is disagree a lot.\n",
        "\n",
        "As an example: \"Does not want alchoholics as neighbors\" --> 1=does not want, 2=doesnt care\n",
        "\n",
        "Or: Trust to the military --> 1=Trust very much, 2= Trust some, 3=Trust a little 4=Does not trust at all "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myBvN4jaDseu"
      },
      "source": [
        "[![2hAEhX.md.png](https://iili.io/2hAEhX.md.png)](https://freeimage.host/i/2hAEhX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTMbEvvDDseu"
      },
      "source": [
        "Pick some varibles you think is interesting and play with creating clusters. Can we explain what is going on?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiQDKkkvDseu"
      },
      "source": [
        "# Your solutions from here..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anz-HSBYDseu"
      },
      "source": [
        "---\n",
        "\n",
        "## Supervised machine learning with penguins\n",
        "\n",
        "This assignment uses the same data as for \"unsupervised machine learning with penguins\". \n",
        "\n",
        "If you created solutions for \"unsupervised machine learning with penguins\", jump to assignment 3.\n",
        "\n",
        "### The assignment\n",
        "\n",
        "1. Inspect the data with some of the standard functions you learned so far (desc, info etc.). What do we have here?\n",
        "2. Likewise, use some standard visualizations (eg. from seaborn) to express some properties of the data\n",
        "3. Apply stanbdard preprocessing (eg. missing values, scaling, outliers, one-hot-encoding)\n",
        "4. Split the data in a train & test sample\n",
        "5. Fit a classification model (target outcome = 'species') on the training data, and evaluate its performance on the test data.\n",
        "   * Use first a logistic regression to do so.\n",
        "   * Then, use 2-3 more complex model classes of your choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MEA42f8Dseu"
      },
      "source": [
        "# standard packaging\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.set(color_codes=True, rc={'figure.figsize':(10,8)})\n",
        "\n",
        "from IPython.display import HTML #Youtube embed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U1yenZcDseu"
      },
      "source": [
        "# load the dataset from GitHub - original source\n",
        "\n",
        "penguins = pd.read_csv(\"https://github.com/allisonhorst/palmerpenguins/raw/5b5891f01b52ae26ad8cb9755ec93672f49328a8/data/penguins_size.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Yfdp-VvDseu"
      },
      "source": [
        "# You solutions from here..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDcrUan2Dseu"
      },
      "source": [
        "## Employee turnover\n",
        "\n",
        "### The assignment\n",
        "\n",
        "In the repo, you will find a dataset describing employee turnover in a company.\n",
        "\n",
        "https://raw.githubusercontent.com/CALDISS-AAU/sdsphd20/master/datasets/turnover.csv\n",
        "\n",
        "The dataset contains data collected in an employee survey and enriched with HR data.\n",
        "\n",
        "The variable `churn` tells us if the employee left the company in the past 3 months. The other variables are collected\n",
        "\n",
        "#### Classification\n",
        "\n",
        "Try to predict `churn` using a classification pipeline (perhaps add some simple exploration of the data first)\n",
        "\n",
        "#### Regression\n",
        "Try to predict the number of weekly average hours worked.\n",
        "\n",
        "**Before** working with the data, you should use `pd.get_dummies` to get dummies for categorical variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Nw_nF0XDseu"
      },
      "source": [
        "# Your solutions from here..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zcz__jbzDseu"
      },
      "source": [
        "---\n",
        "\n",
        "# Portfolio assignments for Wednesday\n",
        "\n",
        "**Requirement:** Work on solutions for *either* the network analysis case study 1 or case study 2 *and* the exercise for spatial stuff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqUNq8XA4Xfb"
      },
      "source": [
        "## Network analysis: Case Study 1: Directed Networks: Friends & Foes at Work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUGY4_tm6gKv"
      },
      "source": [
        "### Introduction to the case\n",
        "\n",
        "* Emmanuel Lazega, The Collegial Phenomenon: The Social Mechanisms of Cooperation Among Peers in a Corporate Law Partnership, Oxford University Press (2001).\n",
        "\n",
        "#### Data \n",
        "This data set comes from a network study of corporate law partnership that was carried out in a Northeastern US corporate law firm, referred to as SG&R, 1988-1991 in New England. It includes (among others) measurements of networks among the 71 attorneys (partners and associates) of this firm, i.e. their strong-coworker network, advice network, friendship network, and indirect control networks. Various members' attributes are also part of the dataset, including seniority, formal status, office in which they work, gender, lawschool attended, individual performance measurements (hours worked, fees brought in), attitudes concerning various management policy options, etc. This dataset was used to identify social processes such as bounded solidarity, lateral control, quality control, knowledge sharing, balancing powers, regulation, etc. among peers.\n",
        "\n",
        "#### Setting\n",
        "* What do corporate lawyers do? Litigation and corporate work.\n",
        "* Division of work and interdependencies.\n",
        "* Three offices, no departments, built-in pressures to grow, intake and assignment rules.\n",
        "* Partners and associates: hierarchy, up or out rule, billing targets.\n",
        "* Partnership agreement (sharing benefits equally, 90% exclusion rule, governance structure, elusive committee system) and incompleteness of the contracts.\n",
        "* Informal, unwritten rules (ex: no moonlighting, no investment in buildings, no nepotism, no borrowing to pay partners, etc.).\n",
        "* Huge incentives to behave opportunistically ; thus the dataset is appropriate for the study of social processes that make cooperation among rival partners possible. \n",
        "* Sociometric name generators used to elicit coworkers, advice, and 'friendship' ties at SG&R:\"Here is the list of all the members of your Firm.\"\n",
        "\n",
        "The networks where created according to the follwoing questionaire:\n",
        "\n",
        "* Strong coworkers network: \"Because most firms like yours are also organized very informally, it is difficult to get a clear idea of how the members really work together. Think back over the past year, consider all the lawyers in your Firm. Would you go through this list and check the names of those with whom you have worked with. By \"worked with\" I mean that you have spent time together on at least one case, that you have been assigned to the same case, that they read or used your work product or that you have read or used their work product; this includes professional work done within the Firm like Bar association work, administration, etc.\"\n",
        "* Basic advice network: \"Think back over the past year, consider all the lawyers in your Firm. To whom did you go for basic professional advice? For instance, you want to make sure that you are handling a case right, making a proper decision, and you want to consult someone whose professional opinions are in general of great value to you. By advice I do not mean simply technical advice.\"\n",
        "* 'Friendship' network:\n",
        "\"Would you go through this list, and check the names of those you socialize with outside work. You know their family, they know yours, for instance. I do not mean all the people you are simply on a friendly level with, or people you happen to meet at Firm functions.\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqZlW3YcDseu"
      },
      "source": [
        "### Data preperation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ac9ru1aDseu"
      },
      "source": [
        "#### Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVpLaHWODseu"
      },
      "source": [
        "# Installing visualization packages\n",
        "!pip install -U bokeh\n",
        "!pip install -q holoviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zK3ODrdDseu"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "import itertools # Python's amazing iteration & combination library\n",
        "import networkx as nx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XEoyW1qDsev"
      },
      "source": [
        "# Visualization defaults\n",
        "import holoviews as hv\n",
        "from holoviews import opts\n",
        "hv.extension('bokeh')\n",
        "from bokeh.plotting import show\n",
        "\n",
        "# Setting the default figure size a bit larger\n",
        "defaults = dict(width=750, height=750, padding=0.1,\n",
        "                xaxis=None, yaxis=None)\n",
        "hv.opts.defaults(\n",
        "    opts.EdgePaths(**defaults), opts.Graph(**defaults), opts.Nodes(**defaults))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if1cPg6k6imS"
      },
      "source": [
        "####  Load the data\n",
        "\n",
        "Lets load the data! The three networks refer to cowork, friendship, and advice. The first 36 respondents are the partners in the firm.\n",
        "\n",
        "(the cell belows reads in the tables, performs some recoding and cleanup and creates network objects for the 3 data sets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71deTXch7iSr"
      },
      "source": [
        "mat_friendship = pd.read_table(\"https://www.dropbox.com/s/0saiulir3pr566k/ELfriend.dat?dl=1\", delim_whitespace=True, header=None) \n",
        "mat_advice = pd.read_table(\"https://www.dropbox.com/s/apq42n1grim23k9/ELadv.dat?dl=1\", delim_whitespace=True, header=None) \n",
        "mat_work = pd.read_table(\"https://www.dropbox.com/s/dliz0sd7or8tv01/ELwork.dat?dl=1\", delim_whitespace=True, header=None)\n",
        "\n",
        "G_friendship = nx.from_pandas_adjacency(mat_friendship, create_using=nx.DiGraph)\n",
        "G_advice = nx.from_pandas_adjacency(mat_advice, create_using=nx.DiGraph)\n",
        "G_work = nx.from_pandas_adjacency(mat_work, create_using=nx.DiGraph)\n",
        "\n",
        "attributes = pd.read_table(\"https://www.dropbox.com/s/qz7fvfgx8lvjgpr/ELattr.dat?dl=1\", delim_whitespace=True, header=None, dtype='int') \n",
        "attributes=attributes.round().astype(int)\n",
        "attributes.columns = [\"id\", \"seniority\", \"gender\", \"office\", \"tenure\", \"age\", \"practice\", \"school\"]\n",
        "attributes.set_index('id',inplace=True)\n",
        "\n",
        "cleanup_nums = {\"seniority\":     {1: \"Partner\", 2: \"Associate\"},\n",
        "                \"gender\":     {1: \"Male\", 2: \"Female\"},\n",
        "                \"office\":     {1: \"Boston\", 2: \"Hartford\", 3:\"Providence\"},\n",
        "                \"practice\":     {1: \"Litigation\", 2: \"Corporate\"},\n",
        "                \"school\":     {1: \"Harvard, Yale\", 2: \"Ucon\", 3: \"Others\"}\n",
        "                } \n",
        "attributes.replace(cleanup_nums, inplace=True)\n",
        "\n",
        "attributes_dict=attributes.T.to_dict()\n",
        "\n",
        "nx.set_node_attributes(G_friendship, attributes_dict)\n",
        "nx.set_node_attributes(G_advice, attributes_dict)\n",
        "nx.set_node_attributes(G_work, attributes_dict)\n",
        "\n",
        "print(nx.get_node_attributes(G_friendship, 'seniority'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWsWIvDeMS3o"
      },
      "source": [
        "#### Calculate dimensional centralities\n",
        "\n",
        "There might be better ways to do that (still experimenting), but for now lets first create centralities upfront for all networks. We for now only look at the in-degree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgkLkBk1YbXS"
      },
      "source": [
        "cent_degree_friendship = dict(G_friendship.in_degree)\n",
        "cent_degree_advice = dict(G_advice.in_degree)\n",
        "cent_degree_work = dict(G_work.in_degree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hJAhy_yY0Xq"
      },
      "source": [
        "nx.set_node_attributes(G_friendship, cent_degree_friendship, 'cent_degree')\n",
        "nx.set_node_attributes(G_advice, cent_degree_advice, 'cent_degree')\n",
        "nx.set_node_attributes(G_work, cent_degree_work, 'cent_degree')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88gPV3VKMxOT"
      },
      "source": [
        "# Create and save a layout.\n",
        "G_layout = nx.layout.kamada_kawai_layout(G_work)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6a5uk1rM8Yl"
      },
      "source": [
        "g_plot = hv.Graph.from_networkx(G_friendship, G_layout).opts(tools=['hover'],\n",
        "                                                                        directed=True,\n",
        "                                                                        edge_alpha=0.25,\n",
        "                                                                        node_size='cent_degree',\n",
        "                                                                        #node_color='seniority', cmap='Set1',\n",
        "                                                                        legend_position='right'\n",
        "                                                                        )\n",
        "\n",
        "show(hv.render(g_plot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXH5FO3BTEKz"
      },
      "source": [
        "g_plot = hv.Graph.from_networkx(G_advice, G_layout).opts(tools=['hover'],\n",
        "                                                                        directed=True,\n",
        "                                                                        edge_alpha=0.25,\n",
        "                                                                        node_size='cent_degree',\n",
        "                                                                        #node_color='cent_degree', cmap='Set1',\n",
        "                                                                        legend_position='right')\n",
        "show(hv.render(g_plot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBtSaFIEcV6D"
      },
      "source": [
        "g_plot = hv.Graph.from_networkx(G_work, G_layout).opts(tools=['hover'],\n",
        "                                                                        directed=True,\n",
        "                                                                        edge_alpha=0.25,\n",
        "                                                                        node_size='cent_degree',\n",
        "                                                                        #node_color='seniority', cmap='Set1',\n",
        "                                                                        legend_position='right')\n",
        "show(hv.render(g_plot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvmBMVQm7vnK"
      },
      "source": [
        "#### Assortiativity\n",
        "\n",
        "We can also calculate another interested measure, particularly in social networks: Assortiativity. In a nutshell, it measures if two nodes that share certain characteristics ahve a higher or lower probability to be connected.\n",
        "\n",
        "For details, check:\n",
        "\n",
        "* Newman, M. E. J. (27 February 2003). \"Mixing patterns in networks\". Physical Review E. American Physical Society (APS). 67 (2): 026126"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWKHXIc5skJL"
      },
      "source": [
        "nx.attribute_assortativity_coefficient(G_friendship, 'seniority')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4gz_Wbg7UZT"
      },
      "source": [
        "nx.attribute_assortativity_coefficient(G_friendship, 'school')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK9n01K47d_D"
      },
      "source": [
        "nx.attribute_assortativity_coefficient(G_friendship, 'office')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbOzP1X_71pH"
      },
      "source": [
        "#### Reciprocity\n",
        "\n",
        "Anotyher interesting question usually is, if directed edges are reciptocated, meaning that an edge between `i,j` makes an edge between `j,i` more likely"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unMN2vQp7_Ka"
      },
      "source": [
        "nx.overall_reciprocity(G_friendship)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8Ats3vnDsew"
      },
      "source": [
        "### The assignment\n",
        "\n",
        "Explore the network further.\n",
        "\n",
        "1. Calculate the reciprocity for the work and advise network. Are the numbers diffetrent? Why might that be the case?\n",
        "2. Identify communities in the friendship and advice network (hint: works only on undirected networks, so you might have to create an undirected graph)\n",
        "3. Visualize these communities (static or dynamic)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnZEIsX5Dsew"
      },
      "source": [
        "# Your solutions here..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2fIywPDDsew"
      },
      "source": [
        "## Network analysis: Case Study 2 in 2-Mode Networks: Exploring Instagram Hashtag Networks\n",
        "\n",
        "In this notebook, we will collect data from Instagram to construct (snowball) a network of hashtags as well as a (2-mode) bipartite network of Instagram users and hashtags.\n",
        "\n",
        "The networks in this example can be considered synthetic, since nodes and particularly edges represent virtual constructs rather than explicit connections.\n",
        "\n",
        "\n",
        "In this case we explore\n",
        "\n",
        "- Graph construction (normal and bipartite)\n",
        "- Calculation of centrality indicators \n",
        "- Community detection\n",
        "- Projection of bipartite network\n",
        "\n",
        "Furthermore you will learn:\n",
        "\n",
        "- to make simple (public) API requests (API: Application Programming Interface) \n",
        "- parse json response\n",
        "- perform simple string manipulation/text-mining to extract features of interest (Transition into NLP)\n",
        "\n",
        "### So what?\n",
        "\n",
        "Such an analysis can be useful in marketing to identify sub-dicussions in a niche or related to a brand. We will detect popular hashtags within sub-niches that \"correlate\" with a topic of interest.\n",
        "Furthermore, we will identify accounts with high engagement (post-counts) within specific hashtag communities.\n",
        "\n",
        "Unfortunately Instagram, very recently (few days back), diesabled a simple public API that allowed to map usernames form user-ids. Therefore, we will use ```instaloader```, a module for interacting with Instagram. \n",
        "\n",
        "We will only use public data that does not require log-in. If you want to explore other graph structures on Instagram (e.g. follow-networks), have a look at Instabot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC69HN1Zj6E-"
      },
      "source": [
        "### Tooling Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewHduCUsDsew"
      },
      "source": [
        "# Installing visualization packages\n",
        "!pip install -U bokeh\n",
        "!pip install -q holoviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh6q6prKDsew"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "import itertools # Python's amazing iteration & combination library\n",
        "import networkx as nx\n",
        "import community\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer # A bit of a transition into NLP. The tweet tokenizer from the NLTK library will help us extract the hashtags from post-text\n",
        "tknzr = TweetTokenizer()\n",
        "import requests as rq # The requests library handles \"requests\" to APIs similar to a browser that requests a webpage given a URL\n",
        "from networkx.algorithms import bipartite # bipartite NW algos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVr9f-Q6Dsew"
      },
      "source": [
        "# Visualization defaults\n",
        "import holoviews as hv\n",
        "from holoviews import opts\n",
        "hv.extension('bokeh')\n",
        "from bokeh.plotting import show\n",
        "\n",
        "# Setting the default figure size a bit larger\n",
        "defaults = dict(width=750, height=750, padding=0.1,\n",
        "                xaxis=None, yaxis=None)\n",
        "hv.opts.defaults(\n",
        "    opts.EdgePaths(**defaults), opts.Graph(**defaults), opts.Nodes(**defaults))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTW9x4HWkzeN"
      },
      "source": [
        "### Getting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXnEZKBBk19D"
      },
      "source": [
        "# Defining global constants for the instagram extract\n",
        "\n",
        "# Note: These things may change without a warning...\n",
        "\n",
        "# Instagram base url preffix\n",
        "tagurl_prefix = 'https://www.instagram.com/explore/tags/'\n",
        "\n",
        "# suffix to append to tag request url to retrieve data in JSON format\n",
        "tagurl_suffix = '/?__a=1'\n",
        "\n",
        "# suffix to end cursor when requesting posts by tag\n",
        "tagurl_endcursor = '&max_id='\n",
        "\n",
        "# a generic media post preffix (concat with media shortcode to view)\n",
        "posturl_prefix = 'https://www.instagram.com/p/'\n",
        "\n",
        "# target initial tags (we will run this with only one tag but the code can take multiple - just extend the list)\n",
        "tags = ['machinelearning']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSDCpB8HlEic"
      },
      "source": [
        "# urls to initial tags using the above url-components\n",
        "queries = [ tagurl_prefix + tag + tagurl_suffix for tag in tags ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsraq-CVlInF"
      },
      "source": [
        "The response structure of this Insta endpoint is not really straightforward. You can read more about it in the original post.\n",
        "The data is most likely composed on request by some large-scale graph database at returned. Instagram obviously assumes that the receiving site is a browser exploring public posts.\n",
        "\n",
        "We also don't get all posts for some hashtag right away but a \"page\" ~25 posts.\n",
        "\n",
        "To receive further posts, we need to pass a new requests specifying \"our position\" by providing an end_cursor.\n",
        "\n",
        "This **end cursor** can be found in\n",
        "\n",
        "```\n",
        "response['graphql']['hashtag']['edge_hashtag_to_media']['page_info']['end_cursor']\n",
        "```\n",
        "\n",
        "#### Some thoughts on JSON\n",
        "\n",
        "This brings us to JSON. Think of JSON objects as of combinations of dictionaries and lists that can contain most Python objects (e.g. lists, dictionaries, tuples, strings, ints etc.) that can be represented as text. Once parsed you can operate JSON objects just as any other dictionary or list in Python.\n",
        "More about JSON - here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxWmgj5Elo8C"
      },
      "source": [
        "edges = []\n",
        "for q in queries:    \n",
        "    for i in range(10): # how many iterations/deepth ?\n",
        "        r = rq.get(q).json()\n",
        "        end_cursor = r['graphql']['hashtag']['edge_hashtag_to_media']['page_info']['end_cursor']\n",
        "        edges.extend(r['graphql']['hashtag']['edge_hashtag_to_media']['edges'])\n",
        "        print(i)\n",
        "        q = q + tagurl_endcursor + end_cursor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kOa1gndmOFn"
      },
      "source": [
        "### Bringing the collected data into useful format...\n",
        "\n",
        "In the next step we will take the rich raw posts data and extract only the information that we need for our analysis. We will just cut out owner-id (account that posted), a shortcode that we can use to identify the post and get more data on it in future if needed, and the text including the hashtags.\n",
        "\n",
        "To make things more compact we not only extract the raw data but we also preprocess a bit.\n",
        "\n",
        "The hashtags are incorporated within the post-text. Therefore, we pass the text of each post through a tokenizer, that identifies individual words and elements (such as emoji). We use the tweet-tokenizer from the NLTK library, which is made for working with social media data.\n",
        "\n",
        "```\n",
        "  tokens = tknzr.tokenize(text)\n",
        "  tags = [x.strip('#') for x in tokens if x.startswith('#')]\n",
        "```\n",
        "\n",
        "The first line turns the text of the post in a list of tokens (words & co.). The second line picks out only the elements that start with a \"#\" and strips the \"#\" when adding them to a list.\n",
        "\n",
        "Then we construct a dictionary with these values and append it to a list.\n",
        "\n",
        "This gives us a list of dicitonaries - something that we can pass to Pandas to get a dataframe we can work with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLB5-d0MmSnJ"
      },
      "source": [
        "post_dicts = [] #empty list\n",
        "\n",
        "for post in edges: #iterate all raw posts\n",
        "\n",
        "    if post['node']['edge_media_to_caption']['edges'] == []: # hop to the next if no text in the post\n",
        "        continue\n",
        "\n",
        "    post_dict = {} # empty dictionary\n",
        "    id_owner = post['node']['owner']['id'] # pick out user-id\n",
        "    shortcode = post['node']['shortcode'] # pick out short post identifier\n",
        "    text = post['node']['edge_media_to_caption']['edges'][0]['node']['text'] # pick out post text\n",
        "\n",
        "    # Pick hashtags from text\n",
        "    tokens = tknzr.tokenize(text)\n",
        "    tags = [x.strip('#') for x in tokens if x.startswith('#')]\n",
        "\n",
        "    # fill in dictionary with values\n",
        "    post_dict['id_owner'] = id_owner\n",
        "    post_dict['shortcode'] = shortcode\n",
        "    post_dict['tags'] = tags\n",
        "    post_dict['text'] = text\n",
        "\n",
        "    post_dicts.append(post_dict) #append the dictionary to a list of post-dictionaries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuFU900mmXR4"
      },
      "source": [
        "# Create DF\n",
        "posts_df = pd.DataFrame(post_dicts)\n",
        "\n",
        "# Remove hashtags that are not a hashtag (emptyspace & mistakes)\n",
        "posts_df['tags'] = posts_df['tags'].map(lambda t: [x for x in t if x.isalnum()])\n",
        "\n",
        "# Kick out posts with 0 hashtags\n",
        "posts_df = posts_df[posts_df['tags'].map(len) != 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4kqyvYhmyd_"
      },
      "source": [
        "### Simple stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JtUU0lpm2GK"
      },
      "source": [
        "# People with most posts (no names though)\n",
        "posts_df['id_owner'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWfAvaXFnSEV"
      },
      "source": [
        "### Creating a graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tvajlu8nV_r"
      },
      "source": [
        "# Create empty undirected Graph\n",
        "G = nx.Graph()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjWyh-drniSY"
      },
      "source": [
        "We will construct the graph from hashtag combinations of each post. We will use `itertools.combinations` for that. Given a list of n objects this will create all possible unique combinations of size k (which we set to 2). Note, that we can build up the Graph sequentially. An edgelist contains all data we need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUCvoXLPnntE"
      },
      "source": [
        "# Create the graph\n",
        "for i in posts_df['tags']:\n",
        "    G.add_edges_from(list(itertools.combinations(i,2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaSxzFs8nr3M"
      },
      "source": [
        "### Preprocessing the Graph\n",
        "\n",
        "It can be a good idea to filter the Graph before analysing. For instance, we can remove all hashtags with low degree-centrality. This can be interpreted as - kicking out made up hashtags or extremely underused ones. We will calculate a percentile threshold and exclude everything under it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blJ6C4AvpGv0"
      },
      "source": [
        "# Calculating degree centrality for the Graph\n",
        "degree_centrality = nx.degree_centrality(G)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAlIKUr3of_O"
      },
      "source": [
        "# Getting a \"reasonable\" lower bound.\n",
        "perc_filter = np.percentile([v for u,v in degree_centrality.items()], 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au-USbGFpc9Q"
      },
      "source": [
        "# Make a subgraph based on nodes with a degree_centrality over the threshold\n",
        "nodes_selected = [x for x,y in degree_centrality.items() if y >= perc_filter]\n",
        "\n",
        "G = G.subgraph(nodes_selected)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooXLo_41phk_"
      },
      "source": [
        "### Analysing the Graph\n",
        "\n",
        "Now we are going to calculate some network indicators and once done, we will export a DataFrame analyse them further."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xwd4O-cppH3"
      },
      "source": [
        "# Recalculate degre-centrality and assign it as a node-attribute\n",
        "degree_centrality = nx.degree_centrality(G)\n",
        "nx.set_node_attributes(G, degree_centrality, 'degree')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MkEa6JTp5R_"
      },
      "source": [
        "# Same for Eigenvector Centrality\n",
        "eigenvector = nx.eigenvector_centrality(G)\n",
        "nx.set_node_attributes(G, eigenvector, 'eigenvector_centrality')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpoDGJWVp9JR"
      },
      "source": [
        "# Same for community detection\n",
        "communities = community.best_partition(G, resolution = 1)\n",
        "nx.set_node_attributes(G, communities, 'community')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP40ZvkKqKjt"
      },
      "source": [
        "graph_df = pd.DataFrame(dict(G.nodes(data=True))).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAv4aUqlqOfG"
      },
      "source": [
        "graph_df['community'].value_counts(normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySIF9zuJqXaV"
      },
      "source": [
        "# Find the 5 most popular hashtags for each identified community\n",
        "tag_per_com = graph_df.groupby('community')['degree'].nlargest(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAP193Fnqa93"
      },
      "source": [
        "tag_per_com[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2t613TVq2gz"
      },
      "source": [
        "### Bipartite graph between users and hashtags\n",
        "\n",
        "Can we identify communities of users given their usage of hashtags?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0Hsp858q7Wl"
      },
      "source": [
        "# Create a new graph\n",
        "B = nx.Graph()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnFCi91Qq-qY"
      },
      "source": [
        "# we will take the same data\n",
        "posts_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5ubVoHqrGwV"
      },
      "source": [
        "# We need to specify the nodes for level 0 - this will be our users\n",
        "B.add_nodes_from(list(set(posts_df.id_owner)), bipartite=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUfNc8fcrMVC"
      },
      "source": [
        "# Then we need to add hashtags nodes as level 1 nodes\n",
        "B.add_nodes_from(list(set(itertools.chain(*posts_df.tags))), bipartite=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cR8aomMrSLo"
      },
      "source": [
        "# This quick loop will generate edges between users and hashtags\n",
        "# Every time someone mentions a #hashtag, a link is created\n",
        "\n",
        "bi_edges = []\n",
        "for i in posts_df[['id_owner','tags']].iterrows(): # we do this row-by-row since each row is a post\n",
        "    id_owner = i[1]['id_owner']\n",
        "    for j in i[1]['tags']:\n",
        "        bi_edges.append((id_owner, j)) # edges are appended to a list as a tuple (id_owner, hashtag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVC_LlLQrVwY"
      },
      "source": [
        "# Let's add the edges to our graph\n",
        "B.add_edges_from(bi_edges)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpCpRAUNraVb"
      },
      "source": [
        "In the next step we will project the graph onto the account-level. For this we need to get the nodesets of the 0 level. We also calculate the level 1 level (just because)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3udCf_grbOx"
      },
      "source": [
        "# Extract a set of nodes with level 0\n",
        "top_nodes = {n for n, d in B.nodes(data=True) if d['bipartite']==0}\n",
        "\n",
        "# the remaining nodes are then level 1\n",
        "bottom_nodes = set(B) - top_nodes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmj9qejNrfeU"
      },
      "source": [
        "# Let's project this graph using a weighted projection\n",
        "G_proj = bipartite.weighted_projected_graph(B, top_nodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMAjRJimrkWO"
      },
      "source": [
        "# Again, we can identify communities\n",
        "bi_communities = community.best_partition(G_proj, resolution = 1)\n",
        "nx.set_node_attributes(G_proj, bi_communities, 'community')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioxp2gqErndc"
      },
      "source": [
        "# Calculate eigenvector centrality and set it as an attribute\n",
        "bi_eigenvector = nx.eigenvector_centrality(G_proj)\n",
        "nx.set_node_attributes(G_proj, bi_eigenvector, 'eigenvector_centrality')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVkT_RngrrLm"
      },
      "source": [
        "# Create a new attribute \"activity\" - or propensity to spam\n",
        "nx.set_node_attributes(G_proj, dict(posts_df.id_owner.value_counts()), 'activity' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8TcvXvIrwY7"
      },
      "source": [
        "# Do spammers connect more in terms of spamming about the same stuff?\n",
        "print(nx.numeric_assortativity_coefficient(G_proj,'activity'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLOQvqbBr3xl"
      },
      "source": [
        "graph_proj_df = pd.DataFrame(dict(G_proj.nodes(data=True))).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPo9xiKbr6XE"
      },
      "source": [
        "graph_proj_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvTnZKd3r8i-"
      },
      "source": [
        "# Find the 5 most central for each identified community\n",
        "user_per_com = graph_proj_df.groupby('community')['eigenvector_centrality'].nlargest(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN1BB-_isAFw"
      },
      "source": [
        "user_per_com"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmlRyNy9ZsiU"
      },
      "source": [
        "### The assignment\n",
        "\n",
        "Basically, try to repeat the exercise on your own with a term of your choice. \n",
        "\n",
        "1. Just run the code from before, where you replace 'machinelearning' with whatever discussion you are interested to map.\n",
        "2. Create the query link as in the notebook.\n",
        "3. Since the instaloader is not working at the moment (seems to be blocked), you have to click on the query link to get the output in your web-browser. C&P it then, and save it in a text file (something like the text editor. that saves unformatted text.)\n",
        "4. Now you can analyse the  instagram network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHOor6pVDsey"
      },
      "source": [
        "## Exercise for spatial stuff\n",
        "\n",
        "So, now I have a better hypothesis; mental health has something to do with noise!\n",
        "\n",
        "Here, you find a new geo-dataset;\n",
        "\n",
        "- OPEN_DATA_STOEJDATA_VIEWPoint.shp\n",
        "\n",
        "This contains information about noise at different places within municipalities. The end goal is to create a map where each point is aggregated to municipality level and we visualize where the noise is more severe. We use the column \"GNSHAST071\" to measure average noice at that point.\n",
        "\n",
        "When that map is done, create a pretty map with the mental health measurement and compare the two - are there any connection between noise at an aggregated level and mental health?\n",
        "\n",
        "Feel free to play around with types of basemaps, types of colors and all sorts of things - the goal is not to become the leading expert in making fancy maps but to have fun and learn stuff."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkc6QeGlDsey"
      },
      "source": [
        "!pip install geopandas # geopandas is not installed by default on Colab - this installs it\n",
        "!pip install contextily # contextily is not installed by default on Colab - this installs it\n",
        "!pip install pygeos # pygeos optional dependency for geopandas (which we will use)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmcwKlvsDsey"
      },
      "source": [
        "# Download and unzip shape files\n",
        "\n",
        "!wget \"https://github.com/CALDISS-AAU/sdsphd20/raw/master/notebooks/wed25/Shapefiles/shapefiles_exercise.zip\"\n",
        "!unzip shapefiles_exercise.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OreYm-p1Dsey"
      },
      "source": [
        "# Loading required packages\n",
        "\n",
        "import geopandas as gdp\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pyproj import Proj\n",
        "import contextily as ctx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_klJsz29Dsey"
      },
      "source": [
        "# Load the data\n",
        "\n",
        "stoejdata = gdp.read_file(\"OPEN_DATA_STOEJDATA_VIEWPoint.shp\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp2YEoqkDsey"
      },
      "source": [
        "# Your solutions from here..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xwi3wsmKDsey"
      },
      "source": [
        "# Portfolio assignments for Thursday\n",
        "\n",
        "**Requirement:** Work on solutions for the \"Trump vs. GPT-2\" assignment\n",
        "\n",
        "## NLP: Trump vs. GPT-2\n",
        "\n",
        "The site [https://faketrump.ai/](https://faketrump.ai/) WAS an interesting example of AI-powered fake-text generation. They wrote in 2019:\n",
        "\n",
        ">We built an artificial intelligence model by fine-tuning [GPT-2](https://openai.com/blog/better-language-models/) to generate tweets in the style of Donald Trumpâ€™s Twitter account. After seeing the results, we also built a discriminator that can accurately detect fake tweets 77% of the time â€” think you can beat our classifier? Try it yourself!\n",
        "\n",
        "Unfortunately, they decided to take down the site and the dataset.\n",
        "\n",
        "GPT-2 is a neural transformer-based model, that has been announced by OpenAI in February 2019 and created considerable discussion because they decided - in contrast to their earlier policies - not to release the mode to the public. Their central argument was that the model could be used to produce fake news, spam and alike too easily. The footnote of the faketrump page reads: â€œGenerating realistic fake text has become much more accessible. We hope to highlight the current state of text generation to demonstrate how difficult it is to discern fiction from reality.â€\n",
        "\n",
        "\n",
        "Since then several organizations and researchers have shown that it is [possible to develop systems to detect â€œfake textâ€](https://www.theguardian.com/technology/2019/jul/04/ai-fake-text-gpt-2-concerns-false-information). We believe that you too can implement a competitive system.\n",
        "\n",
        "Having no dataset from that project, Roman decided to retrain GPT2 to generate new fake trump tweets. If they can do that, we can do that! However, it seems as if it is easier for ML models to identify our fake tweets...well...they are an AI company and probably spent more time on that...\n",
        "\n",
        "> Iâ€™ve just watched Democrats scream over and over again about trying to Impeach the President of the United States. The Impeachment process is a sham.\n",
        "\n",
        "> The Media must understand!â€œThe New York Times is the leader on a very important subject: How to Combat Trump.â€ @foxandfriendsSo pathetic! @foxandfriendsI donâ€™t think so.\n",
        "\n",
        "> He is going to do it soon, and with proper borders. Border security is my top priority.The Democrats have failed the people of Arizona in everything else they have done, even their very good immigration laws. They have no sense.\n",
        "\n",
        "The data can be found [here](https://github.com/SDS-AAU/SDS-master/raw/e2c959494d53859c1844604bed09a28a21566d0f/M3/assignments/trump_vs_GPT2.gz) and has the following format:\n",
        "\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "   <td>0\n",
        "   </td>\n",
        "   <td>1\n",
        "   </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "   <td>string\n",
        "   </td>\n",
        "   <td>boolean\n",
        "   </td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "There are 7368 real Trump tweet and 7368 fake ones.\n",
        "\n",
        "you can open it with:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "data = pd.read_json('https://github.com/SDS-AAU/SDS-master/raw/e2c959494d53859c1844604bed09a28a21566d0f/M3/assignments/trump_vs_GPT2.gz')\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "* Split the data and preprocess it, vectorizing the text using different approaches (BoW, TFIDF, LSI)\n",
        "\n",
        "* Create a system that can identify the fake Trump tweets using LogisticRefression or other classifiers (Sklearn - If you like also more complex models with FastAI, Keras neural nets or alike)\n",
        "\n",
        "* Explore a subset (~1000) of the real and fake tweets using LDA and visualize your exploration\n",
        "\n",
        "* Consider exploring using a different approach (LSI + clustering) or perhaps even [CorEx](https://github.com/gregversteeg/corex_topic)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBcpOT0BGCwv"
      },
      "source": [
        "# Your solutions from here..."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}